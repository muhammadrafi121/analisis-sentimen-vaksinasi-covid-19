{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-08-05T07:20:47.325806Z","iopub.status.busy":"2022-08-05T07:20:47.325030Z","iopub.status.idle":"2022-08-05T07:23:30.037632Z","shell.execute_reply":"2022-08-05T07:23:30.036263Z","shell.execute_reply.started":"2022-08-05T07:20:47.325703Z"},"trusted":true},"outputs":[],"source":["!pip install transformers -U\n","!pip install datasets\n","!pip install nvidia-ml-py3 \n","!pip install humanize\n","!pip install torch -U \n","!pip install transformers[sentencepiece]\n","!pip install -q git+https://github.com/gmihaila/ml_things.git\n","!pip list | grep -E 'transformers|tokenizers'"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-08-05T07:23:30.040982Z","iopub.status.busy":"2022-08-05T07:23:30.040571Z","iopub.status.idle":"2022-08-05T07:23:37.933168Z","shell.execute_reply":"2022-08-05T07:23:37.932197Z","shell.execute_reply.started":"2022-08-05T07:23:30.040941Z"},"trusted":true},"outputs":[],"source":["import os\n","import re\n","import math\n","import torch\n","import pandas as pd\n","import altair as alt\n","from pathlib import Path\n","from tqdm.notebook import tqdm\n","from datasets import load_dataset\n","from altair.utils.data import to_values\n","from torch.utils.data import Dataset, DataLoader\n","from ml_things import plot_dict, plot_confusion_matrix, fix_text\n","from sklearn.metrics import classification_report, accuracy_score\n","from transformers import (\n","    XLNetConfig,\n","    XLNetTokenizer,\n","    XLNetLMHeadModel,\n","    XLNetForSequenceClassification,\n","    Trainer,\n","    TrainingArguments,\n","    AdamW,\n","    get_linear_schedule_with_warmup,\n","    set_seed,\n","    DataCollatorForPermutationLanguageModeling,\n","    )\n","\n","os.environ[\"WANDB_MODE\"] = 'offline'\n","os.environ[\"WANDB_DISABLED\"] = \"true\""]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b62a6a7a-f092-4731-b86a-93dbe988968a","_uuid":"93c43598-70a5-4b03-bb54-e0402d254d98","collapsed":false,"execution":{"iopub.execute_input":"2022-08-05T07:23:37.935333Z","iopub.status.busy":"2022-08-05T07:23:37.934621Z","iopub.status.idle":"2022-08-05T07:23:37.958837Z","shell.execute_reply":"2022-08-05T07:23:37.957575Z","shell.execute_reply.started":"2022-08-05T07:23:37.935291Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["class Pretrain:\n","    def __init__(self, file_path, model_path, checkpoint, output_dir, num_steps):\n","        self.file_path = file_path\n","        self.model_path = model_path\n","        self.checkpoint = checkpoint\n","        self.output_dir = output_dir\n","        self.num_steps = num_steps\n","\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        \n","    \n","    def tokenize_function(self, examples):\n","        # Remove empty lines\n","        examples[\"text\"] = [line for line in examples[\"text\"] if len(line) > 0 and not line.isspace()]\n","        return self.tokenizer(examples[\"text\"], truncation=True, max_length=512)\n","\n","    def group_texts(self, examples):\n","        block_size = 32\n","        # Concatenate all texts.\n","        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","        total_length = len(concatenated_examples[list(examples.keys())[0]])\n","        # We drop the small remainder,\n","        # we could add padding if the model supported it instead of this drop,\n","        # you can customize this part to your needs.\n","        total_length = (total_length // block_size) * block_size\n","        # Split by chunks of max_len.\n","        result = {\n","            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n","            for k, t in concatenated_examples.items()\n","        }\n","        result[\"labels\"] = result[\"input_ids\"].copy()\n","        return result\n","    \n","    def get_datacollator(self):\n","        datasets = load_dataset(\"text\", data_files={\"train\": self.file_path})\n","        \n","        tokenized_datasets = datasets.map(self.tokenize_function, batched=True, num_proc=4, remove_columns=['text'])\n","        self.lm_datasets = tokenized_datasets.map(\n","            self.group_texts,\n","            batched=True,\n","            )\n","\n","        self.data_collator = DataCollatorForPermutationLanguageModeling(tokenizer=self.tokenizer)\n","\n","    def train(self):\n","        os.makedirs(self.output_dir, exist_ok=True)\n","\n","        self.tokenizer = XLNetTokenizer.from_pretrained(self.model_path)\n","        \n","        self.get_datacollator()\n","        \n","        if self.checkpoint == None:\n","            config = XLNetConfig(\n","                n_layer=12,\n","                d_model=768,\n","                n_head=12,\n","                d_inner=4096,\n","                dropout=0.1,\n","                dropatt=0.1,\n","                bi_data=True,\n","                model_type='xlnet',\n","                vocab_size=self.tokenizer.vocab_size,\n","                bos_token_id=self.tokenizer.bos_token_id,\n","                eos_token_id=self.tokenizer.eos_token_id,\n","            )\n","            self.model = XLNetLMHeadModel(config=config)\n","        else:\n","            self.model = XLNetLMHeadModel.from_pretrained(self.checkpoint)\n","        self.model.to(self.device)\n","\n","        self.training_args = TrainingArguments(\n","            output_dir=self.output_dir,\n","            overwrite_output_dir=True,\n","            do_train=True,\n","            num_train_epochs=1,\n","            per_device_train_batch_size=118,\n","            learning_rate=4e-4,\n","            weight_decay=0.01,\n","            max_steps=self.num_steps,\n","            adam_epsilon=1e-6,\n","            warmup_steps=40_000,\n","            save_steps=1011,\n","            save_total_limit=2,\n","            prediction_loss_only=True,\n","        )\n","\n","        self.trainer = Trainer(\n","            model=self.model,\n","            args=self.training_args,\n","            data_collator=self.data_collator,\n","            train_dataset=self.lm_datasets[\"train\"],\n","        )\n","\n","        self.trainer.train(resume_from_checkpoint=self.checkpoint)\n","        self.trainer.save_model(self.output_dir)\n","        self.tokenizer.save_pretrained(self.output_dir)\n","\n","    def showresult(self):\n","        loss_history = {'train_loss': []}\n","        for log_history in self.trainer.state.log_history:\n","            if 'loss' in log_history.keys():\n","                # Deal with trianing loss.\n","                loss_history['train_loss'].append(log_history['loss'])\n","        \n","        source = pd.DataFrame.from_dict(loss_history)\n","        source = source.set_index(pd.RangeIndex(len(loss_history['train_loss']), name='Steps'))\n","        source = source.reset_index().melt('Steps', var_name='category', value_name='Value')\n","        print(source)\n","\n","        alt.Chart(source).mark_line().encode(\n","            x='Steps',\n","            y='Value',\n","            color='category',\n","        )\n","        \n","        \n","    def show_result(self):\n","        # Keep track of train loss.\n","        loss_history = {'train_loss':[]}\n","\n","        # Keep track of train and evaluate perplexity.\n","        # This is a metric useful to track for language models.\n","        perplexity_history = {'train_perplexity':[]}\n","\n","        # Loop through each log history.\n","        for log_history in self.trainer.state.log_history:\n","            if 'loss' in log_history.keys():\n","                # Deal with trianing loss.\n","                loss_history['train_loss'].append(log_history['loss'])\n","                perplexity_history['train_perplexity'].append(math.exp(log_history['loss']))\n","\n","        # Plot Losses.\n","        plot_dict(loss_history, start_step=self.training_args.logging_steps, \n","                  step_size=self.training_args.logging_steps, use_title='Loss', \n","                  use_xlabel='Train Steps', use_ylabel='Values', magnify=2)\n","\n","        print()\n","\n","        # Plot Perplexities.\n","#         plot_dict(perplexity_history, start_step=self.training_args.logging_steps, \n","#                   step_size=self.training_args.logging_steps, use_title='Perplexity', \n","#                   use_xlabel='Train Steps', use_ylabel='Values', magnify=2)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-08-05T07:23:37.961819Z","iopub.status.busy":"2022-08-05T07:23:37.961489Z","iopub.status.idle":"2022-08-05T07:23:37.986193Z","shell.execute_reply":"2022-08-05T07:23:37.985238Z","shell.execute_reply.started":"2022-08-05T07:23:37.961791Z"},"trusted":true},"outputs":[],"source":["class Finetune():\n","    def __init__(self, model_path, train_file, valid_file, batch_size):\n","        self.model_path = model_path\n","        self.train_file = train_file\n","        self.valid_file = valid_file\n","        self.batch_size = batch_size\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        \n","        self.max_length = 512\n","\n","        self.labels_ids = {'negatif': 0, 'positif': 1}\n","        self.n_labels = len(self.labels_ids)\n","        \n","        print('Loading configuraiton...')\n","        self.model_config = XLNetConfig.from_pretrained(pretrained_model_name_or_path=self.model_path, \n","                                                  num_labels=self.n_labels)\n","\n","        # Get model's tokenizer.\n","        print('Loading tokenizer...')\n","        self.tokenizer = XLNetTokenizer.from_pretrained(pretrained_model_name_or_path=self.model_path)\n","\n","        # Get the actual model.\n","        print('Loading model...')\n","        self.model = XLNetForSequenceClassification.from_pretrained(pretrained_model_name_or_path=self.model_path, \n","                                                                   config=self.model_config)\n","\n","        # Load model to defined device.\n","        self.model.to(self.device)\n","        print('Model loaded to `%s`'%self.device)\n","        \n","        print('Dealing with Train...')\n","        # Create pytorch dataset.\n","        train_dataset = CommentsDataset(path=self.train_file, \n","                                       use_tokenizer=self.tokenizer, \n","                                       labels_ids=self.labels_ids,\n","                                       max_sequence_len=self.max_length)\n","        print('Created `train_dataset` with %d examples!'%len(train_dataset))\n","\n","        # Move pytorch dataset into dataloader.\n","        self.train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n","        print('Created `train_dataloader` with %d batches!'%len(self.train_dataloader))\n","\n","        print()\n","\n","        print('Dealing with Validation...')\n","        # Create pytorch dataset.\n","        valid_dataset =  CommentsDataset(path=self.valid_file, \n","                                       use_tokenizer=self.tokenizer, \n","                                       labels_ids=self.labels_ids,\n","                                       max_sequence_len=self.max_length)\n","        print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n","\n","        # Move pytorch dataset into dataloader.\n","        self.valid_dataloader = DataLoader(valid_dataset, batch_size=self.batch_size, shuffle=False)\n","        print('Created `valid_dataloader` with %d batches!'%len(self.valid_dataloader))\n","    \n","    def train(self, optimizer, scheduler):\n","        # Tracking variables.\n","        predictions_labels = []\n","        true_labels = []\n","        # Total loss for this epoch.\n","        total_loss = 0\n","\n","        # Put the model into training mode.\n","        self.model.train()\n","\n","        # For each batch of training data...\n","        for batch in tqdm(self.train_dataloader, total=len(self.train_dataloader)):\n","\n","            # Add original labels - use later for evaluation.\n","            true_labels += batch['labels'].numpy().flatten().tolist()\n","\n","            # move batch to device\n","            batch = {k:v.type(torch.long).to(self.device) for k,v in batch.items()}\n","\n","            self.model.zero_grad()\n","\n","            outputs = self.model(**batch)\n","\n","            loss, logits = outputs[:2]\n","\n","            total_loss += loss.item()\n","\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n","\n","            optimizer.step()\n","\n","            scheduler.step()\n","\n","            logits = logits.detach().cpu().numpy()\n","\n","            predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n","\n","        avg_epoch_loss = total_loss / len(self.train_dataloader)\n","\n","        return true_labels, predictions_labels, avg_epoch_loss\n","\n","    def validation(self):\n","        # Tracking variables\n","        predictions_labels = []\n","        true_labels = []\n","        #total loss for this epoch.\n","        total_loss = 0\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        self.model.eval()\n","\n","        # Evaluate data for one epoch\n","        for batch in tqdm(self.valid_dataloader, total=len(self.valid_dataloader)):\n","            # add original labels\n","            true_labels += batch['labels'].numpy().flatten().tolist()\n","\n","            # move batch to device\n","            batch = {k:v.type(torch.long).to(self.device) for k,v in batch.items()}\n","\n","            with torch.no_grad():        \n","\n","                outputs = self.model(**batch)\n","\n","                loss, logits = outputs[:2]\n","\n","                logits = logits.detach().cpu().numpy()\n","\n","                total_loss += loss.item()\n","\n","                predict_content = logits.argmax(axis=-1).flatten().tolist()\n","\n","                predictions_labels += predict_content\n","\n","            avg_epoch_loss = total_loss / len(self.valid_dataloader)\n","\n","        return true_labels, predictions_labels, avg_epoch_loss\n","    \n","    def showresult(self, data, length):\n","        source = pd.DataFrame.from_dict(data)\n","        source = source.set_index(pd.RangeIndex(length, name='Epochs'))\n","        source = source.reset_index().melt('Epochs', var_name='category', value_name='Value')\n","        print(source)\n","\n","        alt.Chart(source).mark_line().encode(\n","            x='Epochs',\n","            y='Value',\n","            color='category',\n","        )\n","    \n","    def show_result(self, all_loss, all_acc):\n","        # Plot loss curves.\n","        plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'], magnify=0.1)\n","\n","        # Plot accuracy curves.\n","        plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'], magnify=0.1)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-08-05T07:23:37.988342Z","iopub.status.busy":"2022-08-05T07:23:37.987707Z","iopub.status.idle":"2022-08-05T07:23:38.002849Z","shell.execute_reply":"2022-08-05T07:23:38.001786Z","shell.execute_reply.started":"2022-08-05T07:23:37.988306Z"},"trusted":true},"outputs":[],"source":["class CommentsDataset(Dataset):\n","    def __init__(self, path, use_tokenizer, labels_ids, max_sequence_len=None):\n","\n","        # Check if path exists.\n","        if not os.path.exists(path):\n","            # Raise error if path is invalid.\n","            raise ValueError('Invalid `path` variable! Needs to be a directory')\n","        # Check max sequence length.\n","        max_sequence_len = use_tokenizer.max_len if max_sequence_len is None else max_sequence_len\n","        texts = []\n","        labels = []\n","        print('Reading partitions...')\n","        df = pd.read_csv(path, on_bad_lines='error')\n","        df = df.reset_index()  # make sure indexes pair with number of rows\n","        for index, row in df.iterrows():\n","            text = self.clean_text(row['text'])\n","            texts.append(text)\n","            labels.append(labels_ids[row['label']])\n","\n","        # Number of exmaples.\n","        self.n_examples = len(labels)\n","        # Use tokenizer on texts. This can take a while.\n","        print('Using tokenizer on all texts. This can take a while...')\n","        self.inputs = use_tokenizer(\n","            texts, \n","            add_special_tokens=True, \n","            truncation=True, \n","            padding=True, \n","            return_tensors='pt', \n","            max_length=max_sequence_len\n","        )\n","        # Get maximum sequence length.\n","        self.sequence_len = self.inputs['input_ids'].shape[-1]\n","        print('Texts padded or truncated to %d length!' % self.sequence_len)\n","        # Add labels.\n","        self.inputs.update({'labels':torch.tensor(labels)})\n","        print('Finished!\\n')\n","\n","        return\n","    \n","    def clean_text(self, line):\n","        # cleaning wild char except maybe a repetition word\n","        cleanline = re.sub(r\"[^\\w\\s\\-]\", \" \", line).lower()\n","\n","        # cleaning number\n","        cleanline = re.sub('[0-9]', ' ', cleanline)\n","\n","        # cleaning non indonesian character\n","        cleanline = re.sub(r\"[^(a-z)+\\s{1}]\", \"\", cleanline)\n","\n","        # cleaning whitespaces\n","        cleanline = re.sub(r\"\\s+\", \" \", cleanline)\n","        return cleanline\n","\n","    def __len__(self):\n","        return self.n_examples\n","\n","    def __getitem__(self, item):\n","        return {key: self.inputs[key][item] for key in self.inputs.keys()}"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-08-05T07:23:38.004787Z","iopub.status.busy":"2022-08-05T07:23:38.004438Z","iopub.status.idle":"2022-08-05T07:30:52.246651Z","shell.execute_reply":"2022-08-05T07:30:52.245701Z","shell.execute_reply.started":"2022-08-05T07:23:38.004753Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'Pretrain' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12884\\4075182339.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mmain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12884\\4075182339.py\u001b[0m in \u001b[0;36mmain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[1;31m# pre-training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         pretrain = Pretrain(\n\u001b[0m\u001b[0;32m      6\u001b[0m             \u001b[0mfile_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../input/testwiki/wikitest.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m             \u001b[0mmodel_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"../input/xlnet-gpu/my-xlnet-model\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mNameError\u001b[0m: name 'Pretrain' is not defined"]}],"source":["class Main:\n","    def main(self):\n","        \n","        # pre-training\n","        pretrain = Pretrain(\n","            file_path=\"../dataset/training/wiki.txt\",\n","            model_path=\"../pretrained-xlnet-model\", \n","            checkpoint=None,\n","            output_dir=\"../pretrained-xlnet-model\", \n","            num_steps=500000\n","        )\n","        \n","        pretrain.train()\n","        pretrain.showresult()\n","        \n","        # fine-tuning\n","        finetune = Finetune(\n","            model_path='../pretrained-xlnet-model', \n","            train_file='../dataset/training/train.csv', \n","            valid_file='../dataset/training/valid.csv', \n","            batch_size=16\n","        )\n","        \n","        epochs = 3\n","        \n","        optimizer = AdamW(finetune.model.parameters(),\n","                  weight_decay = 0.01,\n","                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n","                  eps = 1e-6 # args.adam_epsilon  - default is 1e-8.\n","                  )\n","\n","        total_steps = len(finetune.train_dataloader) * epochs\n","\n","        # Create the learning rate scheduler.\n","        scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                                    num_warmup_steps = 0, # Default value in run_glue.py\n","                                                    num_training_steps = total_steps)\n","\n","        # Store the average loss after each epoch so we can plot them.\n","        all_loss = {'train_loss':[], 'val_loss':[]}\n","        all_acc = {'train_acc':[], 'val_acc':[]}\n","\n","        # Loop through each epoch.\n","        print('Epoch')\n","        for epoch in tqdm(range(epochs)):\n","            print()\n","            print('Training on batches...')\n","            # Perform one full pass over the training set.\n","            train_labels, train_predict, train_loss = finetune.train(optimizer, scheduler)\n","            train_acc = accuracy_score(train_labels, train_predict)\n","\n","            # Get prediction form model on validation data. \n","            print('Validation on batches...')\n","            valid_labels, valid_predict, val_loss = finetune.validation()\n","            val_acc = accuracy_score(valid_labels, valid_predict)\n","\n","            # Print loss and accuracy values to see how training evolves.\n","            print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n","            print()\n","\n","            # Store the loss value for plotting the learning curve.\n","            all_loss['train_loss'].append(train_loss)\n","            all_loss['val_loss'].append(val_loss)\n","            all_acc['train_acc'].append(train_acc)\n","            all_acc['val_acc'].append(val_acc)\n","            \n","        finetune.show_result(all_loss, all_acc)\n","\n","        finetune.showresult(all_loss, len(all_loss['train_loss']))\n","        finetune.showresult(all_acc, len(all_acc['train_acc']))\n","        \n","        \n","        finetune.model.save_pretrained('../xlnetmodel/model-1')\n","        finetune.tokenizer.save_pretrained('../xlnetmodel/model-1')\n","\n","if __name__ == '__main__':\n","    main = Main()\n","    main.main()"]}],"metadata":{"kernelspec":{"display_name":"Python 3.7.13 ('thesis')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"},"vscode":{"interpreter":{"hash":"2b2ff0c4587cc89d835e9038d3e404a9c4d40bbca03a151b94cc9aa472cc5937"}}},"nbformat":4,"nbformat_minor":4}
