{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T09:55:25.770024Z",
     "iopub.status.busy": "2022-06-27T09:55:25.769437Z",
     "iopub.status.idle": "2022-06-27T09:56:32.090828Z",
     "shell.execute_reply": "2022-06-27T09:56:32.089626Z",
     "shell.execute_reply.started": "2022-06-27T09:55:25.769897Z"
    },
    "id": "MUz_vmR8tyoc",
    "outputId": "f556ba14-f840-4284-8064-ac98174069cb"
   },
   "outputs": [],
   "source": [
    "# Install transformers library.\n",
    "!pip install -q git+https://github.com/huggingface/transformers.git\n",
    "# Install helper functions.\n",
    "!pip install -q git+https://github.com/gmihaila/ml_things.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T09:56:32.09479Z",
     "iopub.status.busy": "2022-06-27T09:56:32.094376Z",
     "iopub.status.idle": "2022-06-27T09:56:40.659566Z",
     "shell.execute_reply": "2022-06-27T09:56:40.658233Z",
     "shell.execute_reply.started": "2022-06-27T09:56:32.094747Z"
    },
    "id": "gjr_J342tOPq"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from ml_things import plot_dict, plot_confusion_matrix, fix_text\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from transformers import (XLNetConfig, \n",
    "                          XLNetForSequenceClassification, \n",
    "                          XLNetTokenizer, AdamW, \n",
    "                          get_linear_schedule_with_warmup,\n",
    "                          set_seed,\n",
    "                          )\n",
    "\n",
    "epochs = 3\n",
    "batch_size = 50\n",
    "max_length = 512\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name_or_path = '../input/my-indo-xlnet-model'\n",
    "\n",
    "labels_ids = {'negatif': 0, 'positif': 1}\n",
    "n_labels = len(labels_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T09:56:40.663006Z",
     "iopub.status.busy": "2022-06-27T09:56:40.66188Z",
     "iopub.status.idle": "2022-06-27T09:56:40.669491Z",
     "shell.execute_reply": "2022-06-27T09:56:40.668183Z",
     "shell.execute_reply.started": "2022-06-27T09:56:40.662963Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(line):\n",
    "    # cleaning wild char except maybe a repetition word\n",
    "    cleanline = re.sub(r\"[^\\w\\s\\-]\", \" \", line).lower()\n",
    "\n",
    "    # cleaning number\n",
    "    cleanline = re.sub('[0-9]', ' ', cleanline)\n",
    "\n",
    "    # cleaning non indonesian character\n",
    "    cleanline = re.sub(r\"[^(a-z)+\\s{1}]\", \"\", cleanline)\n",
    "\n",
    "    # cleaning whitespaces\n",
    "    cleanline = re.sub(r\"\\s+\", \" \", cleanline)\n",
    "    return cleanline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T09:56:40.675417Z",
     "iopub.status.busy": "2022-06-27T09:56:40.67458Z",
     "iopub.status.idle": "2022-06-27T09:56:40.703686Z",
     "shell.execute_reply": "2022-06-27T09:56:40.702648Z",
     "shell.execute_reply.started": "2022-06-27T09:56:40.675376Z"
    },
    "id": "EDEubgJIt23C"
   },
   "outputs": [],
   "source": [
    "class CommentsDataset(Dataset):\n",
    "    def __init__(self, path, use_tokenizer, labels_ids, max_sequence_len=None):\n",
    "\n",
    "        # Check if path exists.\n",
    "        if not os.path.exists(path):\n",
    "            # Raise error if path is invalid.\n",
    "            raise ValueError('Invalid `path` variable! Needs to be a directory')\n",
    "        # Check max sequence length.\n",
    "        max_sequence_len = use_tokenizer.max_len if max_sequence_len is None else max_sequence_len\n",
    "        texts = []\n",
    "        labels = []\n",
    "        print('Reading partitions...')\n",
    "        df = pd.read_csv(path, on_bad_lines='error')\n",
    "        df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "        for index, row in df.iterrows():\n",
    "            text = clean_text(row['text'])\n",
    "            texts.append(text)\n",
    "            labels.append(labels_ids[row['label']])\n",
    "\n",
    "        # Number of exmaples.\n",
    "        self.n_examples = len(labels)\n",
    "        # Use tokenizer on texts. This can take a while.\n",
    "        print('Using tokenizer on all texts. This can take a while...')\n",
    "        self.inputs = use_tokenizer(\n",
    "            texts, \n",
    "            add_special_tokens=True, \n",
    "            truncation=True, \n",
    "            padding=True, \n",
    "            return_tensors='pt', \n",
    "            max_length=max_sequence_len\n",
    "        )\n",
    "        # Get maximum sequence length.\n",
    "        self.sequence_len = self.inputs['input_ids'].shape[-1]\n",
    "        print('Texts padded or truncated to %d length!' % self.sequence_len)\n",
    "        # Add labels.\n",
    "        self.inputs.update({'labels':torch.tensor(labels)})\n",
    "        print('Finished!\\n')\n",
    "\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_examples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return {key: self.inputs[key][item] for key in self.inputs.keys()}\n",
    "\n",
    "\n",
    "\n",
    "def train(dataloader, optimizer_, scheduler_, device_):\n",
    "    # Use global variable for model.\n",
    "    global model\n",
    "\n",
    "    # Tracking variables.\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    # Total loss for this epoch.\n",
    "    total_loss = 0\n",
    "\n",
    "    # Put the model into training mode.\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "\n",
    "        # Add original labels - use later for evaluation.\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        # move batch to device\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss, logits = outputs[:2]\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        predictions_labels += logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "    avg_epoch_loss = total_loss / len(dataloader)\n",
    "\n",
    "    return true_labels, predictions_labels, avg_epoch_loss\n",
    "\n",
    "\n",
    "\n",
    "def validation(dataloader, device_):\n",
    "    # Use global variable for model.\n",
    "    global model\n",
    "    \n",
    "    # Tracking variables\n",
    "    predictions_labels = []\n",
    "    true_labels = []\n",
    "    #total loss for this epoch.\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "    \n",
    "    # Evaluate data for one epoch\n",
    "    for batch in tqdm(dataloader, total=len(dataloader)):\n",
    "        # add original labels\n",
    "        true_labels += batch['labels'].numpy().flatten().tolist()\n",
    "\n",
    "        # move batch to device\n",
    "        batch = {k:v.type(torch.long).to(device_) for k,v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss, logits = outputs[:2]\n",
    "\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predict_content = logits.argmax(axis=-1).flatten().tolist()\n",
    "\n",
    "            predictions_labels += predict_content\n",
    "\n",
    "        avg_epoch_loss = total_loss / len(dataloader)\n",
    "    \n",
    "    return true_labels, predictions_labels, avg_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T09:56:40.706069Z",
     "iopub.status.busy": "2022-06-27T09:56:40.705672Z",
     "iopub.status.idle": "2022-06-27T09:56:53.845315Z",
     "shell.execute_reply": "2022-06-27T09:56:53.844015Z",
     "shell.execute_reply.started": "2022-06-27T09:56:40.70603Z"
    },
    "id": "R4IPr-6LtZNW",
    "outputId": "1d2033a4-fccd-46c7-9151-c16a37b228cb"
   },
   "outputs": [],
   "source": [
    "# Get model configuration.\n",
    "print('Loading configuraiton...')\n",
    "model_config = XLNetConfig.from_pretrained(pretrained_model_name_or_path=model_name_or_path, \n",
    "                                          num_labels=n_labels)\n",
    "\n",
    "# Get model's tokenizer.\n",
    "print('Loading tokenizer...')\n",
    "tokenizer = XLNetTokenizer.from_pretrained(pretrained_model_name_or_path=model_name_or_path)\n",
    "\n",
    "# Get the actual model.\n",
    "print('Loading model...')\n",
    "model = XLNetForSequenceClassification.from_pretrained(pretrained_model_name_or_path=model_name_or_path, \n",
    "                                                           config=model_config)\n",
    "\n",
    "# Load model to defined device.\n",
    "model.to(device)\n",
    "print('Model loaded to `%s`'%device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T09:56:53.84807Z",
     "iopub.status.busy": "2022-06-27T09:56:53.847509Z",
     "iopub.status.idle": "2022-06-27T09:56:55.438018Z",
     "shell.execute_reply": "2022-06-27T09:56:55.436712Z",
     "shell.execute_reply.started": "2022-06-27T09:56:53.848025Z"
    },
    "id": "h01wTzRiu5az",
    "outputId": "a2156216-408c-4d05-d09b-47eea6bd6ef3"
   },
   "outputs": [],
   "source": [
    "print('Dealing with Train...')\n",
    "# Create pytorch dataset.\n",
    "train_dataset = CommentsDataset(path='../input/comments/train.csv', \n",
    "                               use_tokenizer=tokenizer, \n",
    "                               labels_ids=labels_ids,\n",
    "                               max_sequence_len=max_length)\n",
    "print('Created `train_dataset` with %d examples!'%len(train_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "print('Created `train_dataloader` with %d batches!'%len(train_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Validation...')\n",
    "# Create pytorch dataset.\n",
    "valid_dataset =  CommentsDataset(path='../input/comments/valid.csv', \n",
    "                               use_tokenizer=tokenizer, \n",
    "                               labels_ids=labels_ids,\n",
    "                               max_sequence_len=max_length)\n",
    "print('Created `valid_dataset` with %d examples!'%len(valid_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "print('Created `valid_dataloader` with %d batches!'%len(valid_dataloader))\n",
    "\n",
    "print()\n",
    "\n",
    "print('Dealing with Test...')\n",
    "# Create pytorch dataset.\n",
    "test_dataset =  CommentsDataset(path='../input/comments/test.csv', \n",
    "                               use_tokenizer=tokenizer, \n",
    "                               labels_ids=labels_ids,\n",
    "                               max_sequence_len=max_length)\n",
    "print('Created `test_dataset` with %d examples!'%len(test_dataset))\n",
    "\n",
    "# Move pytorch dataset into dataloader.\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print('Created `test_dataloader` with %d batches!'%len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T09:56:55.440895Z",
     "iopub.status.busy": "2022-06-27T09:56:55.439794Z",
     "iopub.status.idle": "2022-06-27T09:59:13.127262Z",
     "shell.execute_reply": "2022-06-27T09:59:13.12614Z",
     "shell.execute_reply.started": "2022-06-27T09:56:55.440851Z"
    },
    "id": "y0UuLX_61Kyl",
    "outputId": "c41635a8-f9cc-4f5e-88a1-0b8604482a1b"
   },
   "outputs": [],
   "source": [
    "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
    "# I believe the 'W' stands for 'Weight Decay fix\"\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  weight_decay = 0.01,\n",
    "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "                  eps = 1e-6 # args.adam_epsilon  - default is 1e-8.\n",
    "                  )\n",
    "\n",
    "# Total number of training steps is number of batches * number of epochs.\n",
    "# `train_dataloader` contains batched data so `len(train_dataloader)` gives \n",
    "# us the number of batches.\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Store the average loss after each epoch so we can plot them.\n",
    "all_loss = {'train_loss':[], 'val_loss':[]}\n",
    "all_acc = {'train_acc':[], 'val_acc':[]}\n",
    "\n",
    "# Loop through each epoch.\n",
    "print('Epoch')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    print()\n",
    "    print('Training on batches...')\n",
    "    # Perform one full pass over the training set.\n",
    "    train_labels, train_predict, train_loss = train(train_dataloader, optimizer, scheduler, device)\n",
    "    train_acc = accuracy_score(train_labels, train_predict)\n",
    "\n",
    "    # Get prediction form model on validation data. \n",
    "    print('Validation on batches...')\n",
    "    valid_labels, valid_predict, val_loss = validation(valid_dataloader, device)\n",
    "    val_acc = accuracy_score(valid_labels, valid_predict)\n",
    "\n",
    "    # Print loss and accuracy values to see how training evolves.\n",
    "    print(\"  train_loss: %.5f - val_loss: %.5f - train_acc: %.5f - valid_acc: %.5f\"%(train_loss, val_loss, train_acc, val_acc))\n",
    "    print()\n",
    "\n",
    "    # Store the loss value for plotting the learning curve.\n",
    "    all_loss['train_loss'].append(train_loss)\n",
    "    all_loss['val_loss'].append(val_loss)\n",
    "    all_acc['train_acc'].append(train_acc)\n",
    "    all_acc['val_acc'].append(val_acc)\n",
    "\n",
    "# Plot loss curves.\n",
    "plot_dict(all_loss, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'], magnify=0.1)\n",
    "\n",
    "# Plot accuracy curves.\n",
    "plot_dict(all_acc, use_xlabel='Epochs', use_ylabel='Value', use_linestyles=['-', '--'], magnify=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T09:59:36.668356Z",
     "iopub.status.busy": "2022-06-27T09:59:36.66793Z",
     "iopub.status.idle": "2022-06-27T09:59:37.719276Z",
     "shell.execute_reply": "2022-06-27T09:59:37.718108Z",
     "shell.execute_reply.started": "2022-06-27T09:59:36.668316Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs('my-model', exist_ok=True)\n",
    "model.save_pretrained('my-model')\n",
    "tokenizer.save_pretrained('my-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-27T09:59:13.12971Z",
     "iopub.status.busy": "2022-06-27T09:59:13.128945Z",
     "iopub.status.idle": "2022-06-27T09:59:36.666157Z",
     "shell.execute_reply": "2022-06-27T09:59:36.6651Z",
     "shell.execute_reply.started": "2022-06-27T09:59:13.129662Z"
    },
    "id": "X6rmJ-tj0mHn",
    "outputId": "0fd235ff-b549-4c93-8e27-2281344a07ad"
   },
   "outputs": [],
   "source": [
    "# Get prediction form model on validation data. This is where you should use\n",
    "# your test data.\n",
    "true_labels, predictions_labels, avg_epoch_loss = validation(test_dataloader, device)\n",
    "\n",
    "# Create the evaluation report.\n",
    "evaluation_report = classification_report(\n",
    "    true_labels, \n",
    "    predictions_labels, \n",
    "    labels=list(labels_ids.values()), \n",
    "    target_names=list(labels_ids.keys())\n",
    ")\n",
    "# Show the evaluation report.\n",
    "print(evaluation_report)\n",
    "\n",
    "# Plot confusion matrix.\n",
    "plot_confusion_matrix(y_true=true_labels, y_pred=predictions_labels, \n",
    "                      classes=list(labels_ids.keys()), normalize=True, \n",
    "                      magnify=0.1,\n",
    "                      );"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
